#! /usr/bin/python import osimport timeimport globimport mathimport itertoolsimport numpy as npimport matplotlib.pyplot as pltfrom sklearn.cross_validation import KFold'''A script that is used to train the model by two kinds of neural network models: one is a single neuron model which takes an input layer and an output layer, the other is a 3-layer neural network model which consists of an input layer, a hidden layer and an output layer. This script serves as a python module and is also executable independently.Version 0.1 by Jie He @LGI2P, EMA'''# default parametersk = 3 # K-fold Cross Validationrescaling_init = 0.1 # rescale the initial weight matricesplot = False # set "True" to visualize the trending of the value of the cost function during the training, attention: the pop-ups of graph must be closed manually to contine until the end.plts = 1000 # how many points to plot in the graph#bias0 = 0. #bias1 = 0.def start_training_kfCV(ts_out_dir, it=2000, alpha=0.02, hidden_dim=3): # return none; ts_out_dir is the fold where sample.mat locates	print '\n\nStep 2: Starting neural network training >>>>>>'	if hidden_dim == 1: HL = False # degrade the 3-layer model to a 2-layer single neuron model	else: HL = True	sm, mns = read_sample_matrix(ts_out_dir+'sample.mat')	kv = KFold(len(sm), n_folds=k, shuffle=True, random_state=None)	c = 0	print 'Loading sample matrix: size',len(sm),'\nModel names:',mns	final_costs = [] 	test_costs = []	test_ms_costs = []	fp = open(ts_out_dir+'synapses.csv','a+')	fp.write('>>>>>New launching\nk=%d it=%d alpha=%f h=%d re_init=%f\n' % (k,it,alpha,hidden_dim,rescaling_init))	fp_result  = open(ts_out_dir+'word_similarity_task_results.csv','a+')	# start weights training	for tr_index, te_index in kv:		c += 1		print '\nK-flod: ',c,'\nTrain index:\n ',tr_index, '\nTest index:\n ', te_index		train_set = [sm[tr] for tr in tr_index]		test_set = [sm[te] for te in te_index]		X = np.array(train_set, dtype='|S4').astype(np.float64)[:, :-1]		X_t = np.array(test_set, dtype='|S4').astype(np.float64)[:, :-1]		y = np.array(train_set, dtype='|S4').astype(np.float64)[:, -1:]		y_t = np.array(test_set, dtype='|S4').astype(np.float64)[:, -1:]	 		synapses, final_cost, test_cost, test_ms_cost, ph, ph2 = simple_neural_network(X, y, X_t, y_t, mns, it=it, alpha=alpha, hd=hidden_dim)		if not HL: np.savetxt(fp, synapses.T, delimiter='\t', header='K-fold: %d\nSynapse 0, transposed (weght matrix from input layer to hidden layer, size of unit number in input layer * 1)'%c, fmt='%.8f')		if HL: 			np.savetxt(fp, synapses[0], delimiter='\t', header='K-fold: %d\nSynapse 0 (weght matrix from input layer to hidden layer, size of unit number in input layer * unit number in hidden layer)'%c, fmt='%.8f')			np.savetxt(fp, synapses[1].T, delimiter='\t', header='K-fold: %d\nSynapse 1, tranposed (weght matrix from hidden layer to output layer, size of unit number in hidden layer * 1)'%c, fmt='%.8f') 			final_costs.append(final_cost)		test_costs.append(test_cost)		test_ms_costs.append(test_ms_cost)	final_mean = [np.mean(final_costs,axis=0)]	test_mean = np.mean(test_costs,axis=0) # zero dimension	test_m_mean = np.mean(test_ms_costs,axis=0) # 1*modelNum, 1 dimension	fp_result.write('\n>>>>>>>>>>>>>>>>\nNew training and test: local time %s\nParameters\t k_fold=%d\t iteration_num=%d\t alpha=%f\t hidden_units=%d\t rescale_init=%f\n'%(time.strftime('%c'),k,it,alpha,hidden_dim,rescaling_init))	#np.savetxt(fp_result, np.array(final_costs), delimiter='\t', header='Final training cost', fmt='%.8f')	np.savetxt(fp_result, final_mean, delimiter='\t', header='Average final training cost', fmt='%.8f')	write_matrix_to_file(fp_result,mns,dimension=1, oneLine=True, fH=True, paraSpace=0)	np.savetxt(fp_result, np.array(test_ms_costs), delimiter='\t', header='Single model cost (average error square)', fmt='%.8f')	np.savetxt(fp_result, test_m_mean.reshape(1,len(test_m_mean)), delimiter='\t', header='Average single model test cost', fmt='%.8f')	np.savetxt(fp_result, np.array(test_costs), delimiter='\t', header='Test cost', fmt='%.8f')	np.savetxt(fp_result, np.array([test_mean]), delimiter='\t', header='Average test cost', fmt='%.8f')	test_m_meanT = test_m_mean.reshape(len(test_m_mean),1)	test_meanT = [[test_mean] for n in mns] # make test_mean a 2-d array so it can be manipulated with the other one	imp = 100*(test_m_meanT - test_meanT)/test_m_meanT # calculate the improvement (the unit is percentage)	mat_value = np.hstack((test_m_meanT, test_meanT, imp))	h = ['Model','MSE Single','MSE Trained','Imp.(%)']	row_h = [[n] for n in mns] + [['Avg.']]		table = np.vstack((mat_value,np.mean(mat_value,axis=0)))	table_final = np.vstack((h, np.hstack((row_h, table))))	np.savetxt(fp_result, table_final, delimiter='\t', header='Results given by MSE', fmt='%s')	fp.close()	fp_result.close()	return Nonedef read_sample_matrix(path): # return sample matrix and model names	sample_matrix = []	with open(path) as sm:		mns = sm.readline().split() # read the model names		for l in sm:			row = []			for v in l.split(): row.append(float(v))			sample_matrix.append(row)	return sample_matrix, mnsdef write_matrix_to_file(filep, matrix, mode='w+', delimiter='\t', paraSpace = 2,dimension=2,oneLine=False, fH=False): 	if not fH: # not file handler		with open(filep, mode) as op: 			for l in matrix: 				if dimension == 2:					for v in l: op.write(str(v)+delimiter)				if dimension == 1: op.write(str(l)+delimiter)				if not oneLine: op.write('\n')			op.write('\n'*(paraSpace+1))	if fH: 		op = filep		for l in matrix: 			if dimension == 2:				paraSpace -= 1 				for v in l: op.write(str(v)+delimiter)			if dimension == 1: op.write(str(l)+delimiter)			if not oneLine: op.write('\n')		op.write('\n'*(paraSpace+1))def simple_neural_network(X, y, X_t, y_t, mns, it=10000, alpha=0.05, hd=3): # return weight matrices(synapses),final cost, test rusults, hidden_dim and rescaling_init	"A simple neural network which consists of an input layer with 6 units(model number) and 1 addittional bias unit, a hidden layer with 4(default) units and a output layer with 1 unit(label). We use the sigmoid function as activation function. All the parameters can be modified in the above line. Note that the bias parameter is ignored due to the vectorization problem (mathmathical problem). "	def calculate_lyers(X,syn0,syn1):		layer_1 = 1/(1+np.exp(-(np.dot(X,syn0))))		#layer_1 = np.hstack([layer_1,X1_column])		layer_2 = 1/(1+np.exp(-(np.dot(layer_1,syn1))))		return layer_1,layer_2	def average_squared_error(a1, a2): # return the error score between the two numpy arrays		if a1.shape != a2.shape:  # shape: samples*1			print 'Data shape error in average error square, please check the data structure: ', a1.shape,a2.shape			exit()		return (sum(v**2 for v in (a1 - a2))/len(a1))[0]	if hd == 1: HL = False # degrade the 3-layer model to a 2-layer single neuron model	else: HL = True	synapse_0 = rescaling_init * (2*np.random.random((len(X[0]), hd))-1)	if HL: synapse_1 = rescaling_init * (2*np.random.random((hd,1)) - 1)	# uncomment the following lines to add the bias	#wm_row_0 = [1. for i in xrange(hd)]	#X_column = [[bias0] for x in xrange(len(X))] 	#X1_column = [[bias1] for x in xrange(len(X))]	#X_t_column = [[bias0] for x in xrange(len(X_t))] 	#wm_row_1 = [1.]	#synapse_0 = np.vstack([synapse_0, wm_row_0])	#synapse_1 = np.vstack([synapse_1, wm_row_1])	#X = np.hstack([X,X_column])	#X_t = np.hstack([X_t,X_t_column])	print 'Initial shapes in neural network:\nsyn0:',synapse_0.shape	if HL: print 'syn1:',synapse_1.shape	print 'X:',X.shape,'\ty:',y.shape	print 'X_t:',X_t.shape,'\ty_t:',y_t.shape	synapses = []	costs = []	ids = []	layer_1 = np.zeros([len(X),hd])	if HL: layer_2 = np.zeros([len(X),1])	for i in xrange(it): # start the iteration, updating the synapses		if HL: layer_1,layer_2 = calculate_lyers(X, synapse_0, synapse_1)		if HL: layer_2_delta = (layer_2 - y)*(layer_2*(1-layer_2))		if not HL: 			layer_1 = 1/(1+np.exp(-(np.dot(X,synapse_0))))			layer_1_delta = (layer_1 - y) * (layer_1 * (1-layer_1))		if HL: layer_1_delta = layer_2_delta.dot(synapse_1.T) * (layer_1 * (1-layer_1))		if HL: synapse_1 -= (alpha * layer_1.T.dot(layer_2_delta))		synapse_0 -= (alpha * X.T.dot(layer_1_delta))				global plts		if it <= plts: plts = it		if i%(it/plts) == 0: 			if HL: costs.append(average_squared_error(layer_2, y))			if not HL: costs.append(average_squared_error(layer_1, y))			ids.append(i) 	if HL: 		final_cost = average_squared_error(layer_2, y)		synapses.append(synapse_0)		synapses.append(synapse_1)	if not HL: 		final_cost = average_squared_error(layer_1, y)		synapses.append(synapse_0)	if HL: l1,l2 = calculate_lyers(X_t, synapse_0, synapse_1)	if not HL: l2 = 1/(1+np.exp(-(np.dot(X_t,synapse_0))))	test_cost =  average_squared_error(l2, y_t)	# test on single model:	test_ms_cost = [] 	for i,m in enumerate(mns):		X_t_m = np.array([X_t[:, i]]).astype(np.float64).T # Must make it a numpy array		cost_m = average_squared_error(y_t, X_t_m)		test_ms_cost.append(cost_m)	if plot:		plt.plot(ids,costs)		plt.xlabel('Iteration')		plt.ylabel('Cost')		plt.axis([-10*it/plts,it,-0.01,0.1])		plt.show()	if not HL: synapses = synapses[0]	return synapses, final_cost, test_cost, test_ms_cost, hd, rescaling_init# some common functions irrelevant to "neural network"def cosine_similarity(v1,v2):    "compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)"    sumxx, sumxy, sumyy = 0, 0, 0    for i in range(len(v1)):        x = v1[i]; y = v2[i]        sumxx += x*x        sumyy += y*y        sumxy += x*y    den = math.sqrt(sumxx*sumyy)    if(den == 0): return 0    return sumxy/den							def end_mark(dirName):	timestr = time.strftime("%d%H%M%S") 	os.rename(dirName, dirName[:-1]+timestr+'_complete/')def get_model_names(sup_dir):	print '\nRetrieving model names >>>>>>'	mns = []	m_dirs = glob.glob('%s*/' % sup_dir)	for n in m_dirs: mns.append(n.replace(sup_dir,'')[:-1])	print 'Model names:', mns	return mnsdef creat_result_folder(prefix_dir_name):	if not os.path.exists('../results/'): os.makedirs('../results/')	timestr = time.strftime("_%m%d%H%M%S_") 	results_dir='../results/%s%s/' % (prefix_dir_name, timestr)	os.makedirs(results_dir)	return results_dirif __name__ == "__main__":    import sys    start_training_kfCV(sys.argv[1])